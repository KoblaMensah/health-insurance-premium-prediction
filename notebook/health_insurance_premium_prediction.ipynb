{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aIaURXd3dFVLUEUJZNkSGoD3vD-BcZY5","timestamp":1756756794734},{"file_id":"1udmwESh5ASOIuvgmpo-EKZWZ_v0HB0PQ","timestamp":1744609339443}],"authorship_tag":"ABX9TyNznrDnm9ZTXXNtvQtNBR8T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"BxlGiYSyO29c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnT-iZ2flKxo"},"outputs":[],"source":["from google.colab import files\n","uploaded = files.upload()"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_absolute_error\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import GridSearchCV\n"],"metadata":{"id":"5YFTBq0JnYcs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv(\"insurance.csv\")\n","df"],"metadata":{"id":"chdcC_Njw1mh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.info())\n","print(df.describe())\n","print(df.describe(include='object')) # included categorical features in the statistical description"],"metadata":{"id":"UjiUV76H3kjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# boxplot to visualize outliers\n","# Select numerical columns\n","numerical_cols = ['age', 'bmi', 'children', 'expenses']\n","\n","# Create boxplots\n","plt.figure(figsize=(12, 8))\n","for i, col in enumerate(numerical_cols):\n","    plt.subplot(2, 2, i+1)\n","    sns.boxplot(y=df[col])\n","    plt.title(f'Outliers in {col}')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"D2ZjXiwJhAly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Expenses vs Number of Children\n","This plot explores how insurance expenses vary based on the number of children. Even though `children` is a numeric column, it behaves like a categorical variable, so we use a boxplot to visualize distribution and outliers.\n"],"metadata":{"id":"AUmmGkk5r1Yu"}},{"cell_type":"code","source":["sns.set(style=\"darkgrid\")\n","sns.pairplot(df)\n","\n","\n","# 1. Distribution of Target Variable (Expenses)\n","plt.figure(figsize=(8, 5))\n","sns.histplot(df['expenses'], kde=True, bins=30)\n","plt.title('Distribution of Insurance Expenses')\n","plt.xlabel('Expenses')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# 2. Correlation Heatmap (Numeric Features)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')\n","plt.title('Correlation Between Numeric Features')\n","plt.show()\n","\n","## Categorical Features vs Expenses (Boxplots)\n","\n","# 1. Smoker vs Expenses\n","plt.figure(figsize=(6, 4))\n","sns.boxplot(x='smoker', y='expenses', data=df)\n","plt.title('Expenses by Smoking Status')\n","plt.show()\n","\n","# 2. Sex vs Expenses\n","plt.figure(figsize=(6, 4))\n","sns.boxplot(x='sex', y='expenses', data=df)\n","plt.title('Expenses by Sex')\n","plt.show()\n","\n","# 3. Region vs Expenses\n","plt.figure(figsize=(8, 5))\n","sns.boxplot(x='region', y='expenses', data=df)\n","plt.title('Expenses by Region')\n","plt.show()\n","\n","\n","# 4. Boxplot: Expenses by Number of Children\n","plt.figure(figsize=(6, 4))\n","sns.boxplot(x='children', y='expenses', data=df)\n","plt.title('Expenses by Number of Children')\n","plt.xlabel('Number of Children')\n","plt.ylabel('Expenses')\n","plt.show()\n","\n","## Continuous Features vs Expenses (Scatterplots)\n","\n","# 1. Age vs Expenses\n","plt.figure(figsize=(6, 4))\n","sns.scatterplot(x='age', y='expenses', data=df)\n","plt.title('Age vs Expenses')\n","plt.show()\n","\n","# 2. BMI vs Expenses\n","plt.figure(figsize=(6, 4))\n","sns.scatterplot(x='bmi', y='expenses', data=df)\n","plt.title('BMI vs Expenses')\n","plt.show()\n","\n","#: Highlight Smokers in BMI vs Expenses\n","plt.figure(figsize=(6, 4))\n","sns.scatterplot(x='bmi', y='expenses', hue='smoker', data=df)\n","plt.title('BMI vs Expenses Colored by Smoker Status')\n","plt.show()"],"metadata":{"id":"FJhVDWgRfNLr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before we proceed to preprocessing, let's explore possible interaction between some of the predictors and plot them against the raw target"],"metadata":{"id":"jclg0XSIlumc"}},{"cell_type":"code","source":["\n","# 1. Encode Categorical Features\n","df_encoded = pd.get_dummies(df, drop_first=True).astype(int)\n","\n","# create interaction terms\n","sns.scatterplot(x=df_encoded['age'] * df_encoded['smoker_yes'], y=df_encoded['expenses'])\n","plt.title('Expenses vs Age * Smoker')\n","plt.xlabel('Age * Smoker')\n","plt.ylabel('Expenses')\n","plt.show()\n","\n","sns.scatterplot(x=df_encoded['age'] * df_encoded['bmi'], y=df_encoded['expenses'])\n","plt.title('Expenses vs Age * BMI')\n","plt.xlabel('Age * BMI')\n","plt.ylabel('Expenses')\n","plt.show()\n","\n","sns.scatterplot(x=df_encoded['age'] * df_encoded['children'], y=df_encoded['expenses'])\n","plt.title('Expenses vs Age * Children')\n","plt.xlabel('Age * Children')\n","plt.ylabel('Expenses')\n","plt.show()\n","\n","sns.scatterplot(x=df_encoded['sex_male'] * df_encoded['smoker_yes'], y=df_encoded['expenses'])\n","plt.title('Expenses vs Sex * Smoker')\n","plt.xlabel('Sex * Smoker')\n","plt.ylabel('Expenses')\n","plt.show()\n","\n","sns.boxplot(x=df_encoded['children'] * df_encoded['smoker_yes'], y=df_encoded['expenses'])\n","plt.title('Expenses by Children-Smoker Interaction')\n","plt.xlabel('Children * Smoker')\n","plt.ylabel('Expenses')\n","plt.show()\n"],"metadata":{"id":"hz7dBaIwltvl","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ploting same interaction terms with the log transformed target."],"metadata":{"id":"Lu9xQFiOuadF"}},{"cell_type":"code","source":["df['log_expenses'] = np.log1p(df['expenses'])\n","\n","# age * smoker VS log_expenses\n","sns.scatterplot(x=df_encoded['age'] * df_encoded['smoker_yes'], y=df['log_expenses'])\n","plt.title('Log Expenses vs Age * Smoker')\n","plt.xlabel('Age * Smoker')\n","plt.ylabel('Log Expenses')\n","plt.show()\n","\n","# age * bmi VS log_expenses\n","sns.scatterplot(x=df_encoded['age'] * df_encoded['bmi'], y=df['log_expenses'])\n","plt.title('Log Expenses vs Age * BMI')\n","plt.xlabel('Age * BMI')\n","plt.ylabel('Log Expenses')\n","plt.show()\n","\n","# age * children VS log_expenses\n","sns.scatterplot(x=df_encoded['age'] * df_encoded['children'], y=df['log_expenses'])\n","plt.title('Log Expenses vs Age * Children')\n","plt.xlabel('Age * Children')\n","plt.ylabel('Log Expenses')\n","plt.show()\n","\n","# sex_male * smoker_yes VS log_expenses\n","sns.scatterplot(x=df_encoded['sex_male'] * df_encoded['smoker_yes'], y=df['log_expenses'])\n","plt.title('Log Expenses vs Sex * Smoker')\n","plt.xlabel('Sex * Smoker')\n","plt.ylabel('Log Expenses')\n","plt.show()\n","\n","# children * smoker_yes VS log expenses\n","sns.boxplot(x=df_encoded['children'] * df_encoded['smoker_yes'], y=df['log_expenses'])\n","plt.title('Log Expenses by Children-Smoker Interaction')\n","plt.xlabel('Children * Smoker')\n","plt.ylabel('Log Expenses')\n","plt.show()"],"metadata":{"id":"lKNc_9cGuZAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After perfoming EDA and learning from all the visualizations above including interaction terms, Data Preprocessing would be done next/below"],"metadata":{"id":"YWdsBETqKxbR"}},{"cell_type":"code","source":["## Preprocessing begins\n","\n","\n","## PREPROCESSING PIPELINE\n","\n","# 1. Encode Categorical Features\n","df_encoded = pd.get_dummies(df, drop_first=True).astype(int)\n","\n","# 2. Created Interaction Features (from EDA insights)\n","df_encoded['bmi_smoker'] = df_encoded['bmi'] * df_encoded['smoker_yes']\n","df_encoded['age_smoker'] = df_encoded['age'] * df_encoded['smoker_yes']\n","df_encoded['age_bmi'] = df_encoded['age'] * df_encoded['bmi']\n","df_encoded['sex_smoker'] = df_encoded['sex_male'] * df_encoded['smoker_yes']\n","df_encoded['children_smoker'] = df_encoded['children'] * df_encoded['smoker_yes']\n","\n","# 3. Log-transform Target (used for models like KNN and Linear Regression only)\n","df_encoded['log_expenses'] = np.log1p(df_encoded['expenses'])  # use np.expm1() to reverse\n","\n","# 4. Define Features and Targets\n","X = df_encoded.drop(columns=['expenses', 'log_expenses'])  # independent variables\n","y_raw = df_encoded['expenses']          # original target\n","y_log = df_encoded['log_expenses']      # log-transformed target\n","\n","# 5. Train-Test Split\n","from sklearn.model_selection import train_test_split\n","\n","X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n","    X, y_raw, test_size=0.2, random_state=42\n",")\n","_, _, y_train_log, y_test_log = train_test_split(\n","    X, y_log, test_size=0.2, random_state=42\n",")\n","\n","# 6. Scale Features (for scaling-sensitive models like KNN, LR)\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_raw)\n","X_test_scaled = scaler.transform(X_test_raw)\n","\n","# 7. Check shapes and previews\n","print(\"X_train_scaled shape:\", X_train_scaled.shape)\n","print(\"X_test_scaled shape:\", X_test_scaled.shape)\n","print(\"Encoded DataFrame shape:\", df_encoded.shape)\n","print(\"X sample:\\n\", X.head().T)\n","print(\"y_raw sample:\\n\", y_raw.head())\n","print(\"y_log sample:\\n\", y_log.head())\n"],"metadata":{"id":"Gqp6Xs05LEhF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, Linear Regression and KNN are built and evaluted"],"metadata":{"id":"JkRtpFPtlEEu"}},{"cell_type":"code","source":["# Model dictionary with their settings\n","models = {\n","    'Linear Regression': {\n","        'model': LinearRegression(),\n","        'X_train': X_train_scaled,\n","        'X_test': X_test_scaled,\n","        'y_train': y_train_log,\n","        'y_test': y_test_raw,  # compare with original after exp transform\n","        'log_target': True\n","    },\n","    'KNN': {\n","        'model': KNeighborsRegressor(n_neighbors=5),\n","        'X_train': X_train_scaled,\n","        'X_test': X_test_scaled,\n","        'y_train': y_train_log,\n","        'y_test': y_test_raw,\n","        'log_target': True\n","    },\n","#     'Decision Tree': {\n","#         'model': DecisionTreeRegressor(random_state=42),\n","#         'X_train': X_train_raw,\n","#         'X_test': X_test_raw,\n","#         'y_train': y_train_raw,\n","#         'y_test': y_test_raw,\n","#         'log_target': False\n","#     },\n","#     'Random Forest': {\n","#         'model': RandomForestRegressor(random_state=42),\n","#         'X_train': X_train_raw,\n","#         'X_test': X_test_raw,\n","#         'y_train': y_train_raw,\n","#         'y_test': y_test_raw,\n","#         'log_target': False\n","#     },\n","#     'Gradient Boosting': {\n","#         'model': GradientBoostingRegressor(random_state=42),\n","#         'X_train': X_train_raw,\n","#         'X_test': X_test_raw,\n","#         'y_train': y_train_raw,\n","#         'y_test': y_test_raw,\n","#         'log_target': False\n","#     },\n","#     'XGBoost': {\n","#         'model': XGBRegressor(random_state=42, verbosity=0),\n","#         'X_train': X_train_raw,\n","#         'X_test': X_test_raw,\n","#         'y_train': y_train_raw,\n","#         'y_test': y_test_raw,\n","#         'log_target': False\n","#     }\n"," }\n","\n","# Evaluation function\n","def evaluate_model(name, model, X_train, X_test, y_train, y_test, log_target=False):\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    # Reverse log transform if needed\n","    if log_target:\n","        y_pred = np.expm1(y_pred)\n","\n","    r2 = r2_score(y_test, y_pred)\n","    mae = mean_absolute_error(y_test, y_pred)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return {\n","        \"Model\": name,\n","        \"R2 Score\": r2,\n","        \"MAE\": mae,\n","        \"RMSE\": rmse\n","    }\n","\n","# Run training and evaluation for all models\n","results = []\n","\n","for name, config in models.items():\n","    result = evaluate_model(\n","        name=name,\n","        model=config['model'],\n","        X_train=config['X_train'],\n","        X_test=config['X_test'],\n","        y_train=config['y_train'],\n","        y_test=config['y_test'],\n","        log_target=config['log_target']\n","    )\n","    results.append(result)\n","\n","# Display results\n","results_df = pd.DataFrame(results).sort_values(by=\"R2 Score\", ascending=False)\n","print(results_df)\n"],"metadata":{"id":"VFWY8rcSSPq1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, I trained and Evaluated all tree based models on raw dataset without interaction but with log tranformed target variable\n"],"metadata":{"id":"9jgdjAOgdbb3"}},{"cell_type":"code","source":["# Step 1: Remove interaction terms\n","interaction_cols = ['bmi_smoker', 'age_smoker', 'age_bmi', 'sex_smoker', 'children_smoker']\n","X_base = df_encoded.drop(columns=['expenses', 'log_expenses'] + interaction_cols)\n","\n","# Step 2: Define log-transformed target\n","y_log = df_encoded['log_expenses']\n","\n","# Step 3: Train-test split for tree models (no interaction terms)\n","\n","X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(\n","    X_base, y_log, test_size=0.2, random_state=42\n",")\n","\n","tree_models = {\n","    'Decision Tree': DecisionTreeRegressor(random_state=42),\n","    'Random Forest': RandomForestRegressor(random_state=42),\n","    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n","    'XGBoost': XGBRegressor(random_state=42, verbosity=0)\n","}\n","\n","# Step 5: Evaluation function (back-transform target for fair comparison)\n","def evaluate_tree_model(name, model, X_train, X_test, y_train_log, y_test_log):\n","    model.fit(X_train, y_train_log)\n","    y_pred_log = model.predict(X_test)\n","\n","    # Reverse log transformation to get predictions on original scale\n","    y_pred_raw = np.expm1(y_pred_log)\n","    y_test_raw = np.expm1(y_test_log)\n","\n","    r2 = r2_score(y_test_raw, y_pred_raw)\n","    mae = mean_absolute_error(y_test_raw, y_pred_raw)\n","    rmse = np.sqrt(mean_squared_error(y_test_raw, y_pred_raw))\n","\n","    return {\n","        \"Model\": name,\n","        \"R2 Score\": r2,\n","        \"MAE\": mae,\n","        \"RMSE\": rmse\n","    }\n","\n","# Step 6: Run training & evaluation\n","tree_results = []\n","\n","for name, model in tree_models.items():\n","    result = evaluate_tree_model(\n","        name=name,\n","        model=model,\n","        X_train=X_train_tree,\n","        X_test=X_test_tree,\n","        y_train_log=y_train_tree,\n","        y_test_log=y_test_tree\n","    )\n","    tree_results.append(result)\n","\n","# Step 7: Display results\n","tree_results_df = pd.DataFrame(tree_results).sort_values(by='R2 Score', ascending=False)\n","print(tree_results_df)\n"],"metadata":{"id":"REgjrF87WHpW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, I trained tree models on raw dataset without interaction terms but also on raw target variable (skewed target)"],"metadata":{"id":"T8UoI8ardsUE"}},{"cell_type":"code","source":["# Step 1: Define base X (no interaction terms)\n","interaction_cols = ['bmi_smoker', 'age_smoker', 'age_bmi', 'sex_smoker', 'children_smoker']\n","X_base = df_encoded.drop(columns=['expenses', 'log_expenses'] + interaction_cols)\n","\n","# Step 2: Define raw target\n","y_raw = df_encoded['expenses']\n","\n","# Step 3: Train-test split\n","\n","X_train_tree_raw, X_test_tree_raw, y_train_tree_raw, y_test_tree_raw = train_test_split(\n","    X_base, y_raw, test_size=0.2, random_state=42\n",")\n","\n","# Step 4: Define tree models\n","\n","tree_models_raw_y = {\n","    'Decision Tree': DecisionTreeRegressor(random_state=42),\n","    'Random Forest': RandomForestRegressor(random_state=42),\n","    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n","    'XGBoost': XGBRegressor(random_state=42, verbosity=0)\n","}\n","\n","# Step 5: Evaluation function (no transformation needed)\n","def evaluate_tree_model_raw(name, model, X_train, X_test, y_train, y_test):\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    r2 = r2_score(y_test, y_pred)\n","    mae = mean_absolute_error(y_test, y_pred)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return {\n","        \"Model\": name,\n","        \"R2 Score\": r2,\n","        \"MAE\": mae,\n","        \"RMSE\": rmse\n","    }\n","\n","# Step 6: Train and evaluate\n","tree_results_raw_y = []\n","\n","for name, model in tree_models_raw_y.items():\n","    result = evaluate_tree_model_raw(\n","        name=name,\n","        model=model,\n","        X_train=X_train_tree_raw,\n","        X_test=X_test_tree_raw,\n","        y_train=y_train_tree_raw,\n","        y_test=y_test_tree_raw\n","    )\n","    tree_results_raw_y.append(result)\n","\n","# Step 7: Display results\n","tree_results_raw_y_df = pd.DataFrame(tree_results_raw_y).sort_values(by='R2 Score', ascending=False)\n","print(tree_results_raw_y_df)\n"],"metadata":{"id":"u4rfpQgcdrwk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Performing Gridsearch on top three models (Gradient Boosting, Random Forest and XGBoost)"],"metadata":{"id":"WEl8KOAzrhPJ"}},{"cell_type":"code","source":["\n","\n","# Define refined search grid\n","grid_params = {\n","    'n_estimators': [100, 150, 200],\n","    'learning_rate': [0.05, 0.1, 0.15],\n","    'max_depth': [3, 6, 10],\n","    'min_samples_split': [2, 4],\n","    'min_samples_leaf': [1, 2],\n","    'subsample': [0.9, 1.0],\n","    'max_features': ['sqrt', 'log2']\n","}\n","\n","# Set up GridSearchCV\n","gb_grid = GridSearchCV(\n","    estimator=GradientBoostingRegressor(random_state=42),\n","    param_grid=grid_params,\n","    cv=5,\n","    scoring='r2',\n","    n_jobs=-1,\n","    verbose=1\n",")\n","\n","# Fit on training data (no log transform)\n","gb_grid.fit(X_train_tree, y_train_tree_raw)\n","\n","# Output results\n","print(\" Gradient Boosting Best Parameters:\", gb_grid.best_params_)\n","print(\" Gradient Boosting Best R² (CV):\", gb_grid.best_score_)\n","\n","\n","\n","# XGBoost: Define refined search grid\n","grid_params_xgb = {\n","    'n_estimators': [50, 150, 200],\n","    'learning_rate': [0.05, 0.1, 0.15],\n","    'max_depth': [3, 6, 10],\n","    'subsample': [0.9, 1.0],\n","    'colsample_bytree': [0.8, 1.0]\n","}\n","\n","# Set up GridSearchCV for XGBoost\n","xgb_grid = GridSearchCV(\n","    estimator=XGBRegressor(random_state=42, verbosity=0),\n","    param_grid=grid_params_xgb,\n","    scoring='r2',\n","    cv=5,\n","    verbose=1,\n","    n_jobs=-1\n",")\n","\n","# Fit XGBoost model on training data (raw predictors and raw target)\n","xgb_grid.fit(X_train_tree, y_train_tree_raw)\n","\n","# Output best results for XGBoost\n","print(\"XGBoost Best Parameters:\", xgb_grid.best_params_)\n","print(\"XGBoost Best R² Score:\", xgb_grid.best_score_)\n","\n","# Random Forest: Define refined search grid\n","\n","grid_params_rf = {\n","    'n_estimators': [50, 150, 200],\n","    'max_depth': [3, 6, 10],\n","    'min_samples_split': [2, 4],\n","    'min_samples_leaf': [1, 2],\n","    'max_features': ['sqrt', 'log2']\n","}\n","\n","# Set up GridSearchCV for Random Forest\n","rf_grid = GridSearchCV(\n","    estimator=RandomForestRegressor(random_state=42),\n","    param_grid=grid_params_rf,\n","    scoring='r2',\n","    cv=5,\n","    verbose=1,\n","    n_jobs=-1\n",")\n","\n","# Fit Random Forest model on training data\n","rf_grid.fit(X_train_tree, y_train_tree_raw)\n","\n","# Output best results for Random Forest\n","print(\"Random Forest Best Parameters:\", rf_grid.best_params_)\n","print(\"Random Forest Best R² Score:\", rf_grid.best_score_)\n","\n","\n"],"metadata":{"id":"qqgklGW91bwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_gb_grid = GradientBoostingRegressor(\n","    learning_rate=0.05,\n","    max_depth=4,\n","    max_features='log2',\n","    min_samples_leaf=2,\n","    min_samples_split=2,\n","    n_estimators=150,\n","    subsample=1.0,\n","    random_state=42\n",")\n","\n","best_gb_grid.fit(X_train_tree, y_train_tree_raw)\n","\n","# eveluate\n","y_pred_grid = best_gb_grid.predict(X_test_tree)\n","\n","r2 = r2_score(y_test_tree_raw, y_pred_grid)\n","mae = mean_absolute_error(y_test_tree_raw, y_pred_grid)\n","rmse = np.sqrt(mean_squared_error(y_test_tree_raw, y_pred_grid))\n","\n","print(\"\\nGradientBoosting Final Model Evaluation:\")\n","print(f\"Test R²: {r2:.4f}\")\n","print(f\"Test MAE: {mae:.2f}\")\n","print(f\"Test RMSE: {rmse:.2f}\")\n","\n","\n","best_rf = RandomForestRegressor(\n","    random_state=42,\n","    max_depth=6,\n","    max_features='log2',\n","    min_samples_leaf=1,\n","    min_samples_split=4,\n","    n_estimators=150\n",")\n","\n","# Retrain on the full training set\n","best_rf.fit(X_train_tree, y_train_tree_raw)\n","\n","# Predict on the test set\n","y_pred_rf = best_rf.predict(X_test_tree)\n","\n","# Evaluate performance\n","r2_rf = r2_score(y_test_tree_raw, y_pred_rf)\n","mae_rf = mean_absolute_error(y_test_tree_raw, y_pred_rf)\n","rmse_rf = np.sqrt(mean_squared_error(y_test_tree_raw, y_pred_rf))\n","\n","print(\"\\nRandom Forest Final Model Evaluation:\")\n","print(f\"Test R²: {r2_rf:.4f}\")\n","print(f\"Test MAE: {mae_rf:.2f}\")\n","print(f\"Test RMSE: {rmse_rf:.2f}\")\n","\n","best_xgb = XGBRegressor(\n","    random_state=42,\n","    colsample_bytree=1.0,\n","    learning_rate=0.07,\n","    max_depth=3,\n","    n_estimators=150,\n","    subsample=1.0,\n","    verbosity=0\n",")\n","\n","# Retrain on the full training set\n","best_xgb.fit(X_train_tree, y_train_tree_raw)\n","\n","# Predict on the test set\n","y_pred_xgb = best_xgb.predict(X_test_tree)\n","\n","# Evaluate performance\n","r2_xgb = r2_score(y_test_tree_raw, y_pred_xgb)\n","mae_xgb = mean_absolute_error(y_test_tree_raw, y_pred_xgb)\n","rmse_xgb = np.sqrt(mean_squared_error(y_test_tree_raw, y_pred_xgb))\n","\n","print(\"\\nXGBoost Final Model Evaluation:\")\n","print(f\"Test R²: {r2_xgb:.4f}\")\n","print(f\"Test MAE: {mae_xgb:.2f}\")\n","print(f\"Test RMSE: {rmse_xgb:.2f}\")\n","\n"],"metadata":{"id":"se6Du3XW54Sy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualizing Feature importance across all three Top Models"],"metadata":{"id":"oY5TD-ZZi7iA"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","\n","features = X_train_tree.columns\n","\n","# Create DataFrames for feature importances for each model\n","df_xgb = pd.DataFrame({\n","    'Feature': features,\n","    'Importance': best_xgb.feature_importances_\n","}).sort_values(by='Importance', ascending=True)\n","\n","df_gb = pd.DataFrame({\n","    'Feature': features,\n","    'Importance': best_gb_grid.feature_importances_\n","}).sort_values(by='Importance', ascending=True)\n","\n","df_rf = pd.DataFrame({\n","    'Feature': features,\n","    'Importance': best_rf.feature_importances_\n","}).sort_values(by='Importance', ascending=True)\n","\n","# Create a figure with three subplots for side-by-side comparison\n","fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n","\n","# XGBoost Feature Importance Plot\n","axes[0].barh(df_xgb['Feature'], df_xgb['Importance'], color='teal')\n","axes[0].set_title(\"XGBoost Feature Importances\")\n","axes[0].set_xlabel(\"Importance\")\n","\n","# Gradient Boosting Feature Importance Plot\n","axes[1].barh(df_gb['Feature'], df_gb['Importance'], color='darkorange')\n","axes[1].set_title(\"Gradient Boosting Feature Importances\")\n","axes[1].set_xlabel(\"Importance\")\n","\n","# Random Forest Feature Importance Plot\n","axes[2].barh(df_rf['Feature'], df_rf['Importance'], color='purple')\n","axes[2].set_title(\"Random Forest Feature Importances\")\n","axes[2].set_xlabel(\"Importance\")\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"5rEsV6wii67J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","\n","joblib.dump(best_xgb, 'xgb_model.pkl')\n","print(\"Final Model saved as 'xgb_model.pkl'\")"],"metadata":{"id":"uWYnA0vkcgVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model = joblib.load('xgb_model.pkl')"],"metadata":{"id":"e9w3X99XwbwF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -lh | grep xgb_model.pkl\n","!ls -lh /content\n","\n"],"metadata":{"id":"Sx00Z20kxNu4"},"execution_count":null,"outputs":[]}]}